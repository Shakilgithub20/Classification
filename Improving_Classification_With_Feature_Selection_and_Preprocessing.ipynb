{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1479_B_Improving_Classification_With_Feature_Selection_and_Preprocessing.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rTz60OBu5Dd",
        "outputId": "55505c06-e676-4a14-c01a-807667d8ecec"
      },
      "source": [
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 22,
          "metadata": {
            "tags": []
          },
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "424bRPyWu_GF"
      },
      "source": [
        "import string\n",
        "import inflect\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.svm import SVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQ6vGycku_Lq",
        "outputId": "be1e3669-da46-4112-c3db-c5b379069141"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbQ67JR9u_Pk",
        "outputId": "c5dbfe68-2421-4a17-ce75-ed882ca8332f"
      },
      "source": [
        "%cd /content/drive/MyDrive/Colab Notebooks/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ErGJmrbuu_S4"
      },
      "source": [
        "def read_files(file_loc):\n",
        "  '''\n",
        "  This function reads tsv data from a file in the drive\n",
        "\n",
        "  args - a string containing the files location\n",
        "  returns - a list containing the text data\n",
        "  '''\n",
        "\n",
        "  dataset = []\n",
        "\n",
        "  with open(file_loc, 'r', encoding='utf-8') as file:\n",
        "    for line in file:\n",
        "      dataset.append(line)\n",
        "\n",
        "  return dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p2K_yb0Su_Vs"
      },
      "source": [
        "def separate_labels(dataset):\n",
        "  '''This function will separate the labels/class and examples/documents from the dataset'''\n",
        "  labels = []\n",
        "  documents = []\n",
        "\n",
        "  for line in dataset:\n",
        "    splitted_line = line.strip().split('\\t', 2)\n",
        "    labels.append(splitted_line[1])\n",
        "    documents.append(splitted_line[2])\n",
        "\n",
        "  return labels, documents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0SaC-8lu_bC"
      },
      "source": [
        "def remove_url(documents):\n",
        "  '''This function removes URL's from Texts'''\n",
        "  url_removed = []\n",
        "\n",
        "  # Your code here\n",
        "  for line in documents:\n",
        "    url_removed.append(re.sub('http[s]?://\\S+', '', line))\n",
        "\n",
        "  return url_removed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOrF4Zcqu_gk"
      },
      "source": [
        "def remove_hashtag(documents):\n",
        "  '''This function will remove all occurences of # from the texts'''\n",
        "  hashtag_removed = []\n",
        "\n",
        "  # map hashtag to space\n",
        "  translator = str.maketrans('#', ' '*len('#'), '')\n",
        "\n",
        "  for line in documents:\n",
        "    hashtag_removed.append(line.translate(translator))\n",
        "\n",
        "  return hashtag_removed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2y3I-ZnQu_kK"
      },
      "source": [
        "def remove_whitespaces(documents):\n",
        "  '''This function removes multiple whitespaces and replace them with a single whitespace'''\n",
        "  whitespace_removed = []\n",
        "\n",
        "  for line in documents:\n",
        "    whitespace_removed.append(' '.join(line.split()))\n",
        "\n",
        "  return whitespace_removed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bVVQiZSEu_nh"
      },
      "source": [
        "def text_lowercasing(documents):\n",
        "  lowercased_docs = []\n",
        "\n",
        "  for line in documents:\n",
        "    lowercased_docs.append(line.lower())\n",
        "\n",
        "  return lowercased_docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXpUjmUeu_qg"
      },
      "source": [
        "def tokenize_sentence(documents):\n",
        "  '''This function takes a line and provides tokens/words by splitting them using NLTK'''\n",
        "  \n",
        "  tokenized_docs = []\n",
        "  \n",
        "  for line in documents:\n",
        "    tokenized_docs.append(word_tokenize(line))\n",
        "\n",
        "  return tokenized_docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4rK6S10Iu_uD"
      },
      "source": [
        "def char_n_gram_ready(documents):\n",
        "  '''This function removes multiple whitespaces and replace them with a single whitespace'''\n",
        "  joined_docs = []\n",
        "\n",
        "  for line in documents:\n",
        "    joined_docs.append(' '.join(line))\n",
        "\n",
        "  return joined_docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1eb18_rxjcS"
      },
      "source": [
        "def remove_punctuation(documents):\n",
        "\n",
        "  punct_removed = []\n",
        "\n",
        "  for doc in documents:\n",
        "    temp = []\n",
        "    for word in doc:\n",
        "      if word not in string.punctuation:\n",
        "        temp.append(word)\n",
        "    \n",
        "    punct_removed.append(temp)\n",
        "\n",
        "  return punct_removed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlNnadApxjr0"
      },
      "source": [
        "def remove_stopwords(documents):\n",
        "  \n",
        "  stopword_removed = []\n",
        "\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "\n",
        "  for doc in documents:\n",
        "    temp = []\n",
        "    for word in doc:\n",
        "      if word not in stop_words:\n",
        "        temp.append(word)\n",
        "    \n",
        "    stopword_removed.append(temp)\n",
        "\n",
        "  return stopword_removed"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Args0j7jxjvW"
      },
      "source": [
        "def apply_stemmer(documents):\n",
        "  stemmed_docs = []\n",
        "  \n",
        "  stemmer = PorterStemmer()\n",
        "\n",
        "  for doc in documents:\n",
        "    stemmed_docs.append([stemmer.stem(plural) for plural in doc])\n",
        "\n",
        "  return stemmed_docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68pybHT9xqse"
      },
      "source": [
        "def identity(X):\n",
        "  return X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uflr_cxhxqvy"
      },
      "source": [
        "def vec_tfidf(tfidf = True):\n",
        "\n",
        "  if tfidf:\n",
        "    vec = TfidfVectorizer(preprocessor = identity, analyzer='word',\n",
        "                          tokenizer = identity, ngram_range = (1,3))\n",
        "    # vec = TfidfVectorizer(preprocessor = identity, \n",
        "    #                       tokenizer = identity)\n",
        "  else:\n",
        "    # vec = CountVectorizer(preprocessor = identity, lowercase=True, analyzer='char',\n",
        "    #                       tokenizer = identity, ngram_range = (2,5))\n",
        "    \n",
        "    vec = CountVectorizer(preprocessor = identity,\n",
        "                          tokenizer = identity)\n",
        "    \n",
        "  return vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhbEcA0gxxYW"
      },
      "source": [
        "def SVM_Static(train_docs, train_lbls, test_docs, test_lbls):\n",
        "\n",
        "  vec = vec_tfidf(tfidf = True)\n",
        "    \n",
        "  # combines the vectorizer with the Naive Bayes classifier\n",
        "  classifier = Pipeline([('vec', vec),\n",
        "                         ('cls', SVC(kernel='linear'))])\n",
        "  \n",
        "  classifier.fit(train_docs, train_lbls)\n",
        "\n",
        "  prediction = classifier.predict(test_docs)\n",
        "\n",
        "  print(\"SVM Accuracy = \", accuracy_score(test_lbls, prediction))\n",
        "  print()\n",
        "\n",
        "  print(classification_report(test_lbls, prediction, labels=classifier.classes_, digits=3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeYsc2G-xxfP"
      },
      "source": [
        "def Naive_Bayes(train_docs, train_lbls, test_docs, test_lbls):\n",
        "\n",
        "  vec = vec_tfidf(tfidf = False)\n",
        "    \n",
        "  # combines the vectorizer with the Naive Bayes classifier\n",
        "  classifier = Pipeline([('vec', vec),\n",
        "                         ('cls', MultinomialNB())])\n",
        "  \n",
        "  classifier.fit(train_docs, train_lbls)\n",
        "\n",
        "  prediction = classifier.predict(test_docs)\n",
        "\n",
        "  print(\"Naive Bayes Accuracy = \", accuracy_score(test_lbls, prediction))\n",
        "  print()\n",
        "\n",
        "  print(classification_report(test_lbls, prediction, labels=classifier.classes_, digits=3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4itQcCVVx1lO"
      },
      "source": [
        "def pre_processing(documents):\n",
        "\n",
        "  documents = remove_url(documents)\n",
        "\n",
        "  # documents = remove_hashtag(documents)\n",
        "\n",
        "  documents = remove_whitespaces(documents)\n",
        "\n",
        "  documents = text_lowercasing(documents)\n",
        "\n",
        "  documents = tokenize_sentence(documents)\n",
        "\n",
        "  documents = remove_punctuation(documents)\n",
        "\n",
        "  documents = remove_stopwords(documents)\n",
        "\n",
        "  documents = apply_stemmer(documents)\n",
        "\n",
        "  # If we use character n_gram you have to enable it | else comment the below line\n",
        "  documents = char_n_gram_ready(documents)\n",
        "\n",
        "  return documents"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "KQ1juHErx2Aa",
        "outputId": "f7e17c6a-3300-46a4-d012-b590d9689e07"
      },
      "source": [
        "def main():\n",
        "  print('Reading The Dataset...')\n",
        "  \n",
        "  # Reading the training data\n",
        "  training_dataset = read_files('/content/drive/MyDrive/Colab Notebooks/corona_data/corona_data/train.tsv')\n",
        "  train_labels, train_docs = separate_labels(training_dataset)\n",
        "\n",
        "  # Reading the test data\n",
        "  test_dataset = read_files('/content/drive/MyDrive/Colab Notebooks/corona_data/corona_data/test.tsv')\n",
        "  test_labels, test_docs = separate_labels(test_dataset)\n",
        "\n",
        "  \n",
        "  # calling the pre processing dunction\n",
        "  train_docs = pre_processing(train_docs)\n",
        "  test_docs = pre_processing(test_docs)\n",
        "  # print(train_docs)\n",
        "\n",
        "  print('\\nTraining the Classifier...')\n",
        "  # Naive_Bayes(train_docs, train_labels, test_docs, test_labels)\n",
        "  SVM_Static(train_docs, train_labels, test_docs, test_labels)\n",
        "\n",
        "  for lbl, doc in zip(train_labels[:5], train_docs[:5]):\n",
        "    print(lbl)\n",
        "    print(doc)\n",
        "    print()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Reading The Dataset...\n",
            "\n",
            "Training the Classifier...\n",
            "SVM Accuracy =  0.49631384939441814\n",
            "\n",
            "                    precision    recall  f1-score   support\n",
            "\n",
            "Extremely Negative      0.557     0.385     0.456       592\n",
            "Extremely Positive      0.661     0.477     0.554       599\n",
            "          Negative      0.459     0.486     0.472      1041\n",
            "           Neutral      0.574     0.543     0.558       619\n",
            "          Positive      0.417     0.559     0.478       947\n",
            "\n",
            "          accuracy                          0.496      3798\n",
            "         macro avg      0.534     0.490     0.504      3798\n",
            "      weighted avg      0.514     0.496     0.498      3798\n",
            "\n",
            "Neutral\n",
            "menyrbi phil_gahan chrisitv\n",
            "\n",
            "Positive\n",
            "advic talk neighbour famili exchang phone number creat contact list phone number neighbour school employ chemist gp set onlin shop account poss adequ suppli regular med order\n",
            "\n",
            "Positive\n",
            "coronaviru australia woolworth give elderli disabl dedic shop hour amid covid-19 outbreak\n",
            "\n",
            "Positive\n",
            "food stock one empti ... pleas n't panic enough food everyon take need stay calm stay safe covid19fr covid_19 covid19 coronaviru confin confinementot confinementgener\n",
            "\n",
            "Extremely Negative\n",
            "readi go supermarket covid19 outbreak 'm paranoid food stock litterali empti coronaviru seriou thing pleas n't panic caus shortag ... coronavirusfr restezchezv stayathom confin\n",
            "\n"
          ]
        }
      ]
    }
  ]
}